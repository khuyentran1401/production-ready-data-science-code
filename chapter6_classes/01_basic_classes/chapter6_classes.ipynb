{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Python Classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IceCream:\n",
    "\tdef __init__(self, flavor: str):\n",
    "\t\tself.flavor = flavor\n",
    "\n",
    "\tdef eat(self):\n",
    "\t\tprint(\n",
    "\t\t\tf\"Eating the {self.flavor} ice cream\"\n",
    "\t\t)\n",
    "\n",
    "\n",
    "chocolate = IceCream(\"chocolate\")\n",
    "vanilla = IceCream(\"vanilla\")\n",
    "\n",
    "chocolate.eat()\n",
    "vanilla.eat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Python Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hide Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Standardizer:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.mean = 0\n",
    "        self.std = 1\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def calculate_mean(self) -> None:\n",
    "        self.mean = np.mean(self.data)\n",
    "\n",
    "    def calculate_std(self) -> None:\n",
    "        self.std = np.std(self.data)\n",
    "\n",
    "    def transform(self) -> np.ndarray:\n",
    "        return (self.data - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Standardizer(np.array([1, 2, 3]))\n",
    "\n",
    "# Users shouldn't need to call these methods\n",
    "s.calculate_mean()\n",
    "s.mean = 10\n",
    "s.calculate_std()\n",
    "\n",
    "# Calling transform() will use the wrong mean and std\n",
    "result = s.transform()\n",
    "print(f\"Unexpected result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self._data = data\n",
    "\n",
    "    def _calculate_mean(self) -> None:\n",
    "        return np.mean(self._data)\n",
    "\n",
    "    def _calculate_std(self) -> None:\n",
    "        return np.std(self._data)\n",
    "\n",
    "    def transform(self) -> np.ndarray:\n",
    "        mean_ = self._calculate_mean()\n",
    "        std = self._calculate_std()\n",
    "        return (self._data - mean_) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Standardizer(np.array([1, 2, 3]))\n",
    "result = s.transform()  # Only expose what users need\n",
    "print(f\"Expected result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Abstract Base Classes for Consistent Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MissingValueHandler:\n",
    "\tdef fill_nulls(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\t\treturn data.fillna(data.mean())\n",
    "\n",
    "\n",
    "class DuplicateHandler:\n",
    "\tdef process_dupes(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\t\treturn data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(\n",
    "\tdata: pd.DataFrame, cleaners: list\n",
    ") -> pd.DataFrame:\n",
    "\tfor cleaner in cleaners:\n",
    "\t\tif isinstance(cleaner, MissingValueHandler):\n",
    "\t\t\tdata = cleaner.fill_nulls(data)\n",
    "\t\telif isinstance(cleaner, DuplicateHandler):\n",
    "\t\t\tdata = cleaner.process_dupes(data)\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierHandler:\n",
    "\tdef process_outliers(\n",
    "\t\tself, data: pd.DataFrame\n",
    "\t) -> pd.DataFrame:\n",
    "\t\tmean = data.mean()\n",
    "\t\tstd = data.std()\n",
    "\t\tz_scores = (data - mean) / std\n",
    "\t\treturn data[z_scores.abs() <= 3]\n",
    "\n",
    "\n",
    "def clean_dataset(\n",
    "\tdata: pd.DataFrame, cleaners: list\n",
    ") -> pd.DataFrame:\n",
    "\tfor cleaner in cleaners:\n",
    "\t\tif isinstance(cleaner, MissingValueHandler):\n",
    "\t\t\tdata = cleaner.fill_nulls(data)\n",
    "\t\telif isinstance(cleaner, DuplicateHandler):\n",
    "\t\t\tdata = cleaner.process_dupes(data)\n",
    "\t\t# Add new cleaner types here\n",
    "\t\telif isinstance(cleaner, OutlierHandler):\n",
    "\t\t\tdata = cleaner.process_outliers(data)\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DataTransformer(ABC):\n",
    "    @abstractmethod\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform the input data\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MissingValueHandler(DataTransformer):\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.fillna(data.mean())\n",
    "\n",
    "\n",
    "class DuplicateRemover(DataTransformer):\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(\n",
    "    data: pd.DataFrame, transformers: List[DataTransformer]\n",
    ") -> pd.DataFrame:\n",
    "    for transformer in transformers:\n",
    "        data = transformer.transform(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame({\"values\": [1, 2, None, 2]})\n",
    "    transformers = [MissingValueHandler(), DuplicateRemover()]\n",
    "    clean_df = clean_dataset(df, transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Composition Over Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class MissingValueHandler:\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"Handle missing values\")\n",
    "        return df.fillna(0)\n",
    "\n",
    "\n",
    "class FeatureScaler(MissingValueHandler):\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = super().process(df)\n",
    "        print(\"Scale numeric features\")\n",
    "        scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "        return scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "class NumericDataProcessor(FeatureScaler):\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = super().process(df)\n",
    "        print(\"Remove duplicates\")\n",
    "        return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "\t{\n",
    "\t\t\"feature1\": [10.5, np.nan, 10.5],\n",
    "\t\t\"feature2\": [100.0, 200.0, 100.0],\n",
    "\t}\n",
    ")\n",
    "\n",
    "processor = NumericDataProcessor()\n",
    "result = processor.process(df)\n",
    "print(\"Result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, steps: List[callable]):\n",
    "        self.steps = steps\n",
    "\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for step in self.steps:\n",
    "            df = step(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Handle missing values\")\n",
    "    return df.fillna(0)\n",
    "\n",
    "\n",
    "def scale_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Scale numeric features\")\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "    return scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Remove duplicates\")\n",
    "    return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "\t{\n",
    "\t\t\"feature1\": [10.5, np.nan, 10.5],\n",
    "\t\t\"feature2\": [100.0, 200.0, 100.0],\n",
    "\t}\n",
    ")\n",
    "\n",
    "\n",
    "# Pipeline without scaling\n",
    "pipeline = DataPipeline(\n",
    "\t[handle_missing_values, remove_duplicates]\n",
    ")\n",
    "result = pipeline.process(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Class Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__str__` and `__repr__` Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetrics:\n",
    "\tdef __init__(self, model_name: str):\n",
    "\t\tself.model_name = model_name\n",
    "\n",
    "\tdef __str__(self) -> str:\n",
    "\t\treturn f\"{self.model_name} Performance\"\n",
    "\n",
    "\tdef __repr__(self) -> str:\n",
    "\t\treturn f\"ModelMetrics(model_name='{self.model_name}')\"\n",
    "\n",
    "\n",
    "rf_metrics = ModelMetrics(\"Random Forest\")\n",
    "print(rf_metrics)\n",
    "print(repr(rf_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__eq__` and `__add__` Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResults:\n",
    "\tdef __init__(self, learning_rate, val_loss):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.val_loss = val_loss\n",
    "\n",
    "\tdef __eq__(self, other):\n",
    "\t\t\"\"\"Check if experiments are similar\"\"\"\n",
    "\t\treturn (\n",
    "\t\t\tabs(self.val_loss - other.val_loss) < 0.01\n",
    "\t\t\tand abs(self.learning_rate - other.learning_rate)\n",
    "\t\t\t< 1e-4\n",
    "\t\t)\n",
    "\n",
    "\tdef __add__(self, other):\n",
    "\t\t\"\"\"Average results of multiple experiment runs\"\"\"\n",
    "\t\treturn ExperimentResults(\n",
    "\t\t\t(self.learning_rate + other.learning_rate) / 2,\n",
    "\t\t\t(self.val_loss + other.val_loss) / 2,\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = ExperimentResults(learning_rate=0.001, val_loss=0.245)\n",
    "exp2 = ExperimentResults(learning_rate=0.001, val_loss=0.248)\n",
    "\n",
    "print(\"Comparisons:\")\n",
    "print(f\"exp1 == exp2: {exp1 == exp2}\")\n",
    "\n",
    "# Average experiments\n",
    "avg_exp = exp1 + exp2\n",
    "print(f\"\\nAverage loss: {avg_exp.val_loss:.3f}\")\n",
    "print(f\"LR: {avg_exp.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    model_name: str\n",
    "    accuracy: str\n",
    "\n",
    "\n",
    "rf_metrics = ModelMetrics(\"Random Forest\", 0.945)\n",
    "print(rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Student:\n",
    "    name: str\n",
    "    grades: List[int] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1 = Student(\"John\")\n",
    "student2 = Student(\"Jane\")\n",
    "\n",
    "# Appending grade to student1\n",
    "student1.grades.append(90)\n",
    "print(student1)\n",
    "\n",
    "# doesn't affect the grades of student2\n",
    "print(student2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DatasetConfig(BaseModel):\n",
    "    dataset_name: str\n",
    "    features: List[str]\n",
    "    target_column: str\n",
    "    train_split: float = Field(gt=0, lt=1)\n",
    "\n",
    "\n",
    "# Using the model in a machine learning pipeline\n",
    "config = DatasetConfig(\n",
    "    dataset_name=\"housing_prices\",\n",
    "    features=[\"sqft\", \"bedrooms\", \"location\"],\n",
    "    target_column=\"price\",\n",
    "    train_split=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classmethod in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame for housing data\n",
    "housing_df = pd.DataFrame(\n",
    "    {\n",
    "        \"price\": [250000, 350000, 450000],\n",
    "        \"area\": [1500, 2000, 2500],\n",
    "        \"bedrooms\": [2, 3, 4],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "housing_df.to_csv(\"data/housing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\tdef __init__(\n",
    "\t\tself, data: pd.DataFrame, name: str, features: List[str]\n",
    "\t):\n",
    "\t\tself.data = data\n",
    "\t\tself.name = name\n",
    "\t\tself.features = features\n",
    "\n",
    "\tdef __str__(self) -> str:\n",
    "\t\treturn (\n",
    "\t\t\tf\"Dataset '{self.name}' with {len(self.features)} \"\n",
    "\t\t\tf\"features and {len(self.data)} samples\"\n",
    "\t\t)\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_csv(cls, filepath: str) -> \"Dataset\":\n",
    "\t\tdata = pd.read_csv(filepath)\n",
    "\t\tname = filepath.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "\t\tfeatures = list(data.columns)\n",
    "\t\treturn cls(data, name, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = Dataset.from_csv(\"data/housing.csv\")\n",
    "print(housing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staticmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, predictions: np.ndarray, actuals: np.ndarray):\n",
    "        self.predictions = predictions\n",
    "        self.actuals = actuals\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_probability(predictions: np.ndarray) -> bool:\n",
    "        \"\"\"Check if predictions are valid probabilities\"\"\"\n",
    "        return all(0 <= p <= 1 for p in predictions)\n",
    "\n",
    "    def calculate_metrics(self) -> dict:\n",
    "        \"\"\"Instance method using static methods\"\"\"\n",
    "        if not self.is_valid_probability(self.predictions):\n",
    "            raise ValueError(\"Invalid prediction probabilities\")\n",
    "        squared_errors = (self.predictions - self.actuals) ** 2\n",
    "        rmse = np.sqrt(np.mean(squared_errors))\n",
    "        return {\"rmse\": round(rmse, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using static methods directly without instance\n",
    "predictions = np.array([0.1, 0.8, 0.3])\n",
    "actuals = np.array([0, 1, 0])\n",
    "\n",
    "is_valid_probabilities = ModelEvaluator.is_valid_probability(\n",
    "\tpredictions\n",
    ")\n",
    "print(f\"Valid probabilities: {is_valid_probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DatasetProfile:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self._data = data\n",
    "        self._sample_size: Optional[int] = None\n",
    "\n",
    "    @property\n",
    "    def sample_size(self) -> Optional[int]:\n",
    "        \"\"\"Getter for sample size\"\"\"\n",
    "        return self._sample_size\n",
    "\n",
    "    @sample_size.setter\n",
    "    def sample_size(self, value: int) -> None:\n",
    "        \"\"\"Setter with validation\"\"\"\n",
    "        if not isinstance(value, int):\n",
    "            raise TypeError(\"Sample size must be an integer\")\n",
    "        if value <= 0 or value > len(self._data):\n",
    "            raise ValueError(\"Invalid sample size\")\n",
    "        self._sample_size = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\": [1, 2, None, 4], \"B\": [5, None, 7, 8]})\n",
    "\n",
    "profile = DatasetProfile(df)\n",
    "\n",
    "# Using setter with validation\n",
    "try:\n",
    "    profile.sample_size = 2\n",
    "    print(f\"Sample size set to: {profile.sample_size}\")\n",
    "\n",
    "    # This will raise an error\n",
    "    profile.sample_size = -1\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slots in Python Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import sys\n",
    "\n",
    "class StandardFeature:\n",
    "\tdef __init__(self, name: str, values: List[float]):\n",
    "\t\tself.name = name\n",
    "\t\tself.values = values\n",
    "\n",
    "\n",
    "class OptimizedFeature:\n",
    "\t__slots__ = [\"name\", \"values\"]\n",
    "\n",
    "\tdef __init__(self, name: str, values: List[float]):\n",
    "\t\tself.name = name\n",
    "\t\tself.values = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Standard class (without slots)\n",
    "std_feature = StandardFeature(\"age\", values)\n",
    "\n",
    "# Dynamic attribute creation works\n",
    "std_feature.new_attr = \"allowed\"\n",
    "\n",
    "# Optimized class (with slots)\n",
    "opt_feature = OptimizedFeature(\"age\", values)\n",
    "\n",
    "# Dynamic attribute creation is not allowed\n",
    "try:\n",
    "\topt_feature.new_attr = \"not allowed\"\n",
    "except AttributeError as e:\n",
    "\tprint(\n",
    "\t\tf\"AttributeError: Cannot add new attributes to slotted class\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison\n",
    "print(\n",
    "\tf\"Memory without slots: {sys.getsizeof(std_feature)} bytes\"\n",
    ")\n",
    "print(f\"Memory with slots: {sys.getsizeof(opt_feature)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Compatible Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, percentile: float = 95):\n",
    "        self.percentile = percentile\n",
    "        self.threshold_: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None):\n",
    "        self.threshold_ = np.percentile(X, self.percentile, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.threshold_ is None:\n",
    "            raise ValueError(\"Fit the transformer first\")\n",
    "        return np.minimum(X, self.threshold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 10], [2, 20], [100, 1000]])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"capper\", OutlierCapper(percentile=75)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "print(\"Transformed data:\\n\", X_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}